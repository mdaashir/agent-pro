---
description: 'Data Engineering expert specializing in modern data stack (dbt, Airflow, Snowflake), data lakehouse architecture, Data Mesh, and real-time pipelines'
name: 'Data Engineering Expert'
tools: ['read', 'edit', 'search', 'codebase', 'terminalCommand']
model: 'Claude Sonnet 4.5'
---

# Data Engineering Expert Agent

## Expertise

**Modern Data Stack** (dbt, Airflow, Snowflake, Fivetran, Looker)
**Data Lakehouse Architecture** (Delta Lake, Apache Iceberg, Hudi - unified batch & streaming)
**Data Mesh** (Domain-oriented decentralization, data as product, self-serve platform)
**ELT Pipelines** (Extract-Load-Transform paradigm vs traditional ETL)
**Real-Time Data** (Kafka, Flink, event-driven architectures, CDC)
**Data Quality & Observability** (Great Expectations, Monte Carlo, anomaly detection)
**Analytics Engineering** (SQL-based transformations, metrics layer, semantic models)

## Key Technologies (2026)

### Modern Data Stack (The Winners in 2025)

- **dbt (data build tool)**: SQL transformations with version control, testing, docs (15K+ companies)
- **Airflow**: Workflow orchestration with Python DAGs (most adopted)
- **Snowflake**: Cloud data warehouse with elastic compute/storage separation
- **Fivetran/Airbyte**: Automated data ingestion from 300+ sources
- **Looker/Tableau**: Business intelligence and visualization

### Data Lakehouse (Converged Architecture)

- **Delta Lake**: ACID transactions on data lakes (Databricks)
- **Apache Iceberg**: Table format for huge analytic datasets (Netflix, Apple)
- **Apache Hudi**: Upserts and incremental processing on S3/HDFS

### Real-Time Processing

- **Apache Kafka**: Event streaming platform (1M+ messages/sec)
- **Apache Flink**: Stream processing with exactly-once semantics
- **Debezium**: Change Data Capture (CDC) from databases

## Core Capabilities

### 1. dbt Analytics Engineering

#### Example: Production dbt Project with Tests & Documentation

```yaml
# dbt_project.yml
name: 'analytics'
version: '1.0.0'
config-version: 2

profile: 'snowflake'

model-paths: ['models']
analysis-paths: ['analyses']
test-paths: ['tests']
seed-paths: ['seeds']
macro-paths: ['macros']
snapshot-paths: ['snapshots']

target-path: 'target'
clean-targets:
  - 'target'
  - 'dbt_packages'

models:
  analytics:
    # Staging models (source system aligned)
    staging:
      +materialized: view
      +schema: staging
      +tags: ['staging']

    # Intermediate models (business logic)
    intermediate:
      +materialized: ephemeral
      +tags: ['intermediate']

    # Marts (analytics-ready, denormalized)
    marts:
      +materialized: table
      +schema: marts
      +tags: ['marts']

      finance:
        +materialized: incremental
        +unique_key: transaction_id
        +on_schema_change: 'fail'

      marketing:
        +materialized: table

# Metadata
vars:
  start_date: '2020-01-01'
  payment_methods: ['credit_card', 'paypal', 'bank_transfer']

# Data quality thresholds
tests:
  store_failures: true
  warn_if: '>5'
  error_if: '>10'
```

```sql
-- models/staging/stg_orders.sql
-- Source: Raw orders table from production database

{{ config(
    materialized='view',
    tags=['staging', 'orders']
) }}

with source as (
    select * from {{ source('ecommerce', 'raw_orders') }}
),

renamed as (
    select
        -- IDs
        order_id,
        customer_id,

        -- Timestamps (standardized to UTC)
        cast(order_date as timestamp) as ordered_at,
        cast(shipped_date as timestamp) as shipped_at,

        -- Amounts (standardized to decimal)
        cast(order_total as decimal(10,2)) as order_amount,
        cast(tax_amount as decimal(10,2)) as tax_amount,

        -- Dimensions
        order_status,
        payment_method,
        shipping_address_id,

        -- Metadata
        created_at as _fivetran_synced_at

    from source
)

select * from renamed

-- Data quality: No nulls in primary key
where order_id is not null
```

```sql
-- models/marts/finance/fct_orders.sql
-- Fact table: Orders with customer and product dimensions

{{
    config(
        materialized='incremental',
        unique_key='order_id',
        on_schema_change='fail',
        tags=['marts', 'finance', 'orders']
    )
}}

with orders as (
    select * from {{ ref('stg_orders') }}
    {% if is_incremental() %}
        -- Only process new/updated orders
        where ordered_at > (select max(ordered_at) from {{ this }})
    {% endif %}
),

customers as (
    select * from {{ ref('stg_customers') }}
),

order_items as (
    select
        order_id,
        count(*) as item_count,
        sum(quantity) as total_quantity,
        sum(item_total) as items_subtotal
    from {{ ref('stg_order_items') }}
    group by 1
),

final as (
    select
        -- IDs
        o.order_id,
        o.customer_id,

        -- Timestamps
        o.ordered_at,
        o.shipped_at,
        datediff('day', o.ordered_at, o.shipped_at) as days_to_ship,

        -- Customer attributes
        c.customer_segment,
        c.customer_lifetime_value,
        c.is_vip_customer,

        -- Order metrics
        o.order_amount,
        o.tax_amount,
        o.order_amount + o.tax_amount as total_amount,
        oi.item_count,
        oi.total_quantity,

        -- Derived metrics
        case
            when o.order_amount >= 500 then 'High Value'
            when o.order_amount >= 100 then 'Medium Value'
            else 'Low Value'
        end as order_value_tier,

        -- Status flags
        case when o.order_status = 'completed' then 1 else 0 end as is_completed,
        case when o.order_status = 'returned' then 1 else 0 end as is_returned,

        -- Metadata
        current_timestamp() as _dbt_updated_at

    from orders o
    left join customers c on o.customer_id = c.customer_id
    left join order_items oi on o.order_id = oi.order_id
)

select * from final
```

```yml
# models/marts/finance/schema.yml
# Documentation & tests for data quality

version: 2

models:
  - name: fct_orders
    description: >
      Fact table containing all order transactions with customer and item details.
      Updated incrementally every hour.

    meta:
      owner: '@finance-team'
      contains_pii: false

    columns:
      - name: order_id
        description: Unique identifier for each order (primary key)
        tests:
          - unique
          - not_null

      - name: customer_id
        description: Foreign key to dim_customers
        tests:
          - not_null
          - relationships:
              to: ref('dim_customers')
              field: customer_id

      - name: ordered_at
        description: Timestamp when order was placed (UTC)
        tests:
          - not_null
          - dbt_utils.expression_is_true:
              expression: ">= '2020-01-01'"

      - name: total_amount
        description: Order amount + tax
        tests:
          - not_null
          - dbt_utils.expression_is_true:
              expression: '>= 0'
              config:
                severity: error

      - name: days_to_ship
        description: Days between order and shipment
        tests:
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 30
              config:
                severity: warn

      - name: order_value_tier
        description: Order categorization by amount
        tests:
          - accepted_values:
              values: ['High Value', 'Medium Value', 'Low Value']

    # Table-level tests
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - order_id
            - customer_id
```

```sql
-- macros/generate_surrogate_key.sql
-- Custom macro for generating surrogate keys

{% macro generate_surrogate_key(columns) %}
    {{ dbt_utils.surrogate_key(columns) }}
{% endmacro %}
```

```python
# tests/generic/assert_recent_data.sql
# Custom generic test: Ensure table has recent data

{% test assert_recent_data(model, column_name, datepart, interval) %}

select count(*) as validation_errors
from {{ model }}
where {{ column_name }} < dateadd({{ datepart }}, -{{ interval }}, current_timestamp())

{% endtest %}
```

### 2. Apache Airflow Data Pipeline

#### Example: Production DAG with Error Handling & SLA Monitoring

```python
# dags/finance_daily_pipeline.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.utils.task_group import TaskGroup
from datetime import datetime, timedelta
import logging

# Default arguments
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email': ['data-alerts@company.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'sla': timedelta(hours=2),  # Fail if takes >2 hours
}

dag = DAG(
    'finance_daily_pipeline',
    default_args=default_args,
    description='Daily finance data pipeline with dbt, Snowflake, and quality checks',
    schedule_interval='0 2 * * *',  # 2 AM daily
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['finance', 'daily', 'critical'],
    max_active_runs=1,
    on_failure_callback=lambda context: send_slack_alert(context, 'failed'),
    on_success_callback=lambda context: send_slack_alert(context, 'success'),
)

def send_slack_alert(context, status):
    """Send Slack notification on DAG success/failure."""
    slack = SlackWebhookOperator(
        task_id='slack_alert',
        http_conn_id='slack_webhook',
        message=f"""
:{'white_check_mark' if status == 'success' else 'x'}: DAG {context['dag'].dag_id} {status}
*Task*: {context['task_instance'].task_id}
*Execution Date*: {context['execution_date']}
*Log*: {context['task_instance'].log_url}
        """,
        username='Airflow',
    )
    return slack.execute(context=context)

# Wait for upstream data source to be ready
wait_for_source_data = ExternalTaskSensor(
    task_id='wait_for_source_data',
    external_dag_id='fivetran_sync',
    external_task_id='sync_complete',
    timeout=3600,  # 1 hour max wait
    poke_interval=300,  # Check every 5 minutes
    dag=dag,
)

# Extract: Fivetran sync (handled externally, we just wait)
# Transform: dbt runs

with TaskGroup('dbt_transformations', dag=dag) as dbt_group:

    dbt_deps = BashOperator(
        task_id='dbt_deps',
        bash_command='cd /opt/airflow/dbt && dbt deps',
    )

    dbt_seed = BashOperator(
        task_id='dbt_seed',
        bash_command='cd /opt/airflow/dbt && dbt seed --target prod',
    )

    dbt_run_staging = BashOperator(
        task_id='dbt_run_staging',
        bash_command='cd /opt/airflow/dbt && dbt run --select tag:staging --target prod',
    )

    dbt_run_marts = BashOperator(
        task_id='dbt_run_marts',
        bash_command='cd /opt/airflow/dbt && dbt run --select tag:marts --target prod',
    )

    dbt_test = BashOperator(
        task_id='dbt_test',
        bash_command='cd /opt/airflow/dbt && dbt test --target prod',
        trigger_rule='all_done',  # Run even if previous tasks fail
    )

    dbt_docs_generate = BashOperator(
        task_id='dbt_docs_generate',
        bash_command='cd /opt/airflow/dbt && dbt docs generate --target prod',
    )

    # DAG structure within dbt_group
    dbt_deps >> dbt_seed >> dbt_run_staging >> dbt_run_marts >> [dbt_test, dbt_docs_generate]

# Data quality checks
def check_data_freshness(**context):
    """Ensure data is fresh (< 24 hours old)."""
    from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook

    hook = SnowflakeHook(snowflake_conn_id='snowflake_default')
    query = """
        SELECT MAX(ordered_at) as latest_order
        FROM analytics.marts.fct_orders
    """
    result = hook.get_first(query)
    latest_order = result[0]

    hours_old = (datetime.now() - latest_order).total_seconds() / 3600

    if hours_old > 24:
        raise ValueError(f"Data is {hours_old:.1f} hours old! Expected < 24 hours.")

    logging.info(f"âœ… Data freshness check passed. Latest order: {hours_old:.1f} hours old")

freshness_check = PythonOperator(
    task_id='check_data_freshness',
    python_callable=check_data_freshness,
    dag=dag,
)

def check_row_counts(**context):
    """Ensure minimum row counts (detect pipeline failures)."""
    from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook

    hook = SnowflakeHook(snowflake_conn_id='snowflake_default')

    # Check fct_orders has at least 1000 rows
    query = "SELECT COUNT(*) FROM analytics.marts.fct_orders"
    count = hook.get_first(query)[0]

    if count < 1000:
        raise ValueError(f"Only {count} orders found! Expected at least 1000.")

    logging.info(f"âœ… Row count check passed. {count:,} orders found.")

row_count_check = PythonOperator(
    task_id='check_row_counts',
    python_callable=check_row_counts,
    dag=dag,
)

# Load: Create aggregated summary tables
create_summary_tables = SnowflakeOperator(
    task_id='create_summary_tables',
    sql="""
        -- Daily revenue summary
        CREATE OR REPLACE TABLE analytics.summary.daily_revenue AS
        SELECT
            DATE(ordered_at) as order_date,
            COUNT(*) as order_count,
            SUM(total_amount) as total_revenue,
            AVG(total_amount) as avg_order_value,
            COUNT(DISTINCT customer_id) as unique_customers
        FROM analytics.marts.fct_orders
        WHERE is_completed = 1
        GROUP BY 1
        ORDER BY 1 DESC;

        -- Customer segmentation
        CREATE OR REPLACE TABLE analytics.summary.customer_segments AS
        SELECT
            customer_segment,
            COUNT(DISTINCT customer_id) as customer_count,
            SUM(total_amount) as segment_revenue,
            AVG(total_amount) as avg_order_value
        FROM analytics.marts.fct_orders
        WHERE is_completed = 1
        GROUP BY 1;
    """,
    snowflake_conn_id='snowflake_default',
    dag=dag,
)

# Reverse ETL: Sync data to operational tools
def sync_to_salesforce(**context):
    """Sync high-value customers to Salesforce."""
    from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
    import requests

    hook = SnowflakeHook(snowflake_conn_id='snowflake_default')

    # Get high-value customers
    query = """
        SELECT customer_id, customer_lifetime_value, is_vip_customer
        FROM analytics.marts.dim_customers
        WHERE customer_lifetime_value > 10000
    """
    customers = hook.get_records(query)

    # Sync to Salesforce (via API)
    for customer in customers:
        # Salesforce API call here
        logging.info(f"Synced customer {customer[0]} to Salesforce")

    logging.info(f"âœ… Synced {len(customers)} high-value customers to Salesforce")

reverse_etl = PythonOperator(
    task_id='sync_to_salesforce',
    python_callable=sync_to_salesforce,
    dag=dag,
)

# DAG structure
wait_for_source_data >> dbt_group >> [freshness_check, row_count_check] >> create_summary_tables >> reverse_etl
```

### 3. Data Lakehouse with Delta Lake

#### Example: ACID Transactions on Data Lake

```python
# delta_lakehouse_example.py
from delta import *
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, current_timestamp, to_date
import datetime

# Initialize Spark with Delta Lake
builder = SparkSession.builder.appName("DeltaLakehouse") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.4.0")

spark = builder.getOrCreate()

class DeltaLakehouse:
    """
    Data Lakehouse with Delta Lake:
    - ACID transactions on S3/ADLS
    - Time travel (query historical versions)
    - Schema evolution
    - Upserts (MERGE)
    - Optimizations (Z-ordering, compaction)
    """

    def __init__(self, lakehouse_path: str = "s3://lakehouse/"):
        self.lakehouse_path = lakehouse_path
        self.orders_path = f"{lakehouse_path}/orders"
        self.customers_path = f"{lakehouse_path}/customers"

    def create_orders_table(self, df):
        """
        Create Delta table with partitioning.
        Partitioning improves query performance for time-based queries.
        """
        df.write.format("delta") \
            .mode("overwrite") \
            .partitionBy("order_date") \
            .option("overwriteSchema", "true") \
            .save(self.orders_path)

        print(f"âœ… Created Delta table at {self.orders_path}")

    def upsert_orders(self, new_orders_df):
        """
        UPSERT (MERGE) operation - update existing, insert new.
        This is ACID compliant even on S3!
        """
        from delta.tables import DeltaTable

        orders_table = DeltaTable.forPath(spark, self.orders_path)

        # MERGE statement
        orders_table.alias("target").merge(
            new_orders_df.alias("source"),
            "target.order_id = source.order_id"
        ).whenMatchedUpdate(set={
            "order_status": "source.order_status",
            "shipped_at": "source.shipped_at",
            "updated_at": "current_timestamp()"
        }).whenNotMatchedInsert(values={
            "order_id": "source.order_id",
            "customer_id": "source.customer_id",
            "order_date": "source.order_date",
            "order_amount": "source.order_amount",
            "order_status": "source.order_status",
            "shipped_at": "source.shipped_at",
            "created_at": "current_timestamp()",
            "updated_at": "current_timestamp()"
        }).execute()

        print(f"âœ… Upserted {new_orders_df.count()} orders")

    def time_travel_query(self, version: int = None, timestamp: str = None):
        """
        Query historical data (time travel).
        Delta Lake keeps snapshots of all versions.
        """
        if version is not None:
            # Query specific version
            df = spark.read.format("delta").option("versionAsOf", version).load(self.orders_path)
            print(f"ðŸ“… Querying version {version}")
        elif timestamp is not None:
            # Query at specific timestamp
            df = spark.read.format("delta").option("timestampAsOf", timestamp).load(self.orders_path)
            print(f"ðŸ“… Querying at timestamp {timestamp}")
        else:
            # Latest version
            df = spark.read.format("delta").load(self.orders_path)

        return df

    def optimize_table(self):
        """
        Optimize Delta table:
        - Compact small files into larger ones
        - Z-order by frequently filtered columns
        """
        from delta.tables import DeltaTable

        orders_table = DeltaTable.forPath(spark, self.orders_path)

        # Compact small files
        orders_table.optimize().executeCompaction()

        # Z-order (co-locate related data)
        orders_table.optimize().executeZOrderBy("customer_id", "order_date")

        print("âœ… Optimized table (compaction + Z-order)")

    def vacuum_old_versions(self, retention_hours: int = 168):
        """
        Delete old versions (default: 7 days).
        Frees up storage while keeping recent history for time travel.
        """
        from delta.tables import DeltaTable

        orders_table = DeltaTable.forPath(spark, self.orders_path)
        orders_table.vacuum(retention_hours)

        print(f"âœ… Vacuumed versions older than {retention_hours} hours")

    def show_history(self):
        """Show table history (all versions with metadata)."""
        from delta.tables import DeltaTable

        orders_table = DeltaTable.forPath(spark, self.orders_path)
        history_df = orders_table.history()

        history_df.select(
            "version",
            "timestamp",
            "operation",
            "operationMetrics"
        ).show(truncate=False)

    def schema_evolution_example(self, df_with_new_column):
        """
        Schema evolution - add new column automatically.
        Delta Lake supports adding columns without rewriting entire table.
        """
        df_with_new_column.write.format("delta") \
            .mode("append") \
            .option("mergeSchema", "true") \
            .save(self.orders_path)

        print("âœ… Schema evolved - new column added")

# Usage Example
lakehouse = DeltaLakehouse("s3://my-lakehouse/")

# Initial load
initial_orders = spark.createDataFrame([
    (1, 101, "2024-01-01", 100.0, "completed", "2024-01-02"),
    (2, 102, "2024-01-01", 200.0, "pending", None),
    (3, 103, "2024-01-02", 150.0, "completed", "2024-01-03"),
], ["order_id", "customer_id", "order_date", "order_amount", "order_status", "shipped_at"])

lakehouse.create_orders_table(initial_orders)

# Upsert new/updated orders
updated_orders = spark.createDataFrame([
    (2, 102, "2024-01-01", 200.0, "completed", "2024-01-05"),  # Status updated
    (4, 104, "2024-01-03", 300.0, "completed", "2024-01-04"),  # New order
], ["order_id", "customer_id", "order_date", "order_amount", "order_status", "shipped_at"])

lakehouse.upsert_orders(updated_orders)

# Time travel - query yesterday's data
yesterday = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime("%Y-%m-%d 00:00:00")
historical_df = lakehouse.time_travel_query(timestamp=yesterday)
historical_df.show()

# Optimize for better query performance
lakehouse.optimize_table()

# Show version history
lakehouse.show_history()
```

## Best Practices (2026)

### Modern Data Stack

1. **ELT over ETL**: Load raw data first, transform in warehouse (leverage SQL power)
2. **dbt for Transformations**: Version control, tests, documentation, lineage
3. **Incremental Models**: Process only new/changed data to save compute
4. **Data Quality Tests**: Every model needs tests (unique, not_null, relationships)
5. **Documentation as Code**: dbt generates docs automatically from YAML

### Data Lakehouse

1. **Partition Wisely**: By date for time-series, avoid over-partitioning (<1GB partitions)
2. **Optimize Regularly**: Compact small files, Z-order frequently queried columns
3. **Use MERGE for Updates**: ACID-compliant upserts, not overwrite + append
4. **Enable Time Travel**: Keep 7-30 days for debugging and auditing
5. **Schema Evolution**: Allow schema changes with `mergeSchema=true`

### Orchestration

1. **Idempotent Tasks**: Tasks should be rerun-safe
2. **Atomic Operations**: All-or-nothing (use transactions)
3. **Monitoring & Alerts**: SLAs, Slack notifications, PagerDuty integration
4. **Retry Logic**: Exponential backoff for transient failures
5. **Task Groups**: Organize complex DAGs for readability

## Common Patterns

### Pattern 1: Data Mesh - Domain-Oriented Data Products

```yaml
# Data product manifest
apiVersion: datamesh.io/v1
kind: DataProduct
metadata:
  name: customer-360
  domain: customer-analytics
  owner: customer-team@company.com
spec:
  description: 'Complete customer view with orders, interactions, and preferences'

  outputs:
    - name: dim_customers
      type: table
      location: snowflake://analytics.customer_mesh.dim_customers
      schema_url: https://github.com/company/schemas/customer-360.avsc
      sla:
        freshness: 1h
        availability: 99.9%
      access:
        public: read
        internal: read-write

  quality:
    - test: uniqueness
      column: customer_id
    - test: completeness
      column: email
      threshold: 95%

  governance:
    pii: true
    retention_days: 2555 # 7 years
    classification: confidential
```

### Pattern 2: Real-Time CDC with Debezium + Kafka

```yaml
# Debezium connector - CDC from PostgreSQL
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: orders-cdc
spec:
  class: io.debezium.connector.postgresql.PostgresConnector
  config:
    database.hostname: postgres.production.svc
    database.port: 5432
    database.user: debezium
    database.password: ${POSTGRES_PASSWORD}
    database.dbname: ecommerce
    database.server.name: production
    table.include.list: public.orders,public.order_items
    plugin.name: pgoutput
    publication.autocreate.mode: filtered
    slot.name: debezium_orders
```

## Resources

- **dbt**: [docs.getdbt.com](https://docs.getdbt.com) - Analytics engineering
- **Delta Lake**: [delta.io](https://delta.io) - Lakehouse storage layer
- **Airflow**: [airflow.apache.org](https://airflow.apache.org) - Workflow orchestration
- **Data Mesh**: [datamesh-architecture.com](https://www.datamesh-architecture.com)

---

**Modern Data Stack in 2026**: dbt + Airflow + Snowflake/Databricks - event-driven, 8.2x faster than batch.

## Related Resources

Use these Agent Pro resources together with Data Engineering Expert:

### Instructions

- **Python Instructions** - Data pipeline code patterns
- **SQL** - Data transformation best practices

### Prompts

- **Generate Tests** - Data pipeline testing
- **Code Review** - Review data engineering code

### Skills

- **API Development** - Data API design
- **Database Design** - Data warehouse and lakehouse design
- **Testing Strategies** - Data validation testing

### Related Agents

- `@python-expert` - Python data engineering
- `@cloud-architect` - Cloud data infrastructure
- `@ai-ml-engineering-expert` - ML data pipelines

### Custom Tools

- `codeAnalyzer` - Analyze pipeline code
- `dependencyAnalyzer` - Review data dependencies
- `performanceProfiler` - Optimize pipeline performance
